{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eb6e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc444a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocess.Preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f60d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "for file in files:\n",
    "    with open(os.path.join(config.text_file_location, file),\"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.readlines()\n",
    "    messages.extend(parse_and_merge_messages(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f6ed57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1...\n",
      "Processing chunk 2...\n",
      "Processing chunk 3...\n",
      "Processing chunk 4...\n",
      "Processing chunk 5...\n",
      "Processing chunk 6...\n",
      "Processing chunk 7...\n",
      "Processing chunk 8...\n",
      "Processing chunk 9...\n",
      "Processing chunk 10...\n",
      "Processing chunk 11...\n",
      "Processing chunk 12...\n",
      "Processing chunk 13...\n",
      "Processing chunk 14...\n",
      "Processing chunk 15...\n",
      "Processing chunk 16...\n",
      "Processing chunk 17...\n",
      "Processing chunk 18...\n",
      "Processing chunk 19...\n",
      "Processing chunk 20...\n",
      "Processing chunk 21...\n",
      "Processing chunk 22...\n",
      "Processing chunk 23...\n",
      "Processing chunk 24...\n",
      "Processing chunk 25...\n",
      "Processing chunk 26...\n",
      "Processing chunk 27...\n",
      "Processing chunk 28...\n",
      "Processing chunk 29...\n",
      "Processing chunk 30...\n",
      "Processing chunk 31...\n",
      "Processing chunk 32...\n",
      "Processing chunk 33...\n",
      "Processing chunk 34...\n",
      "Processing chunk 35...\n",
      "Processing chunk 36...\n",
      "Processing chunk 37...\n",
      "Processing chunk 38...\n",
      "Processing chunk 39...\n",
      "Processing chunk 40...\n",
      "Processing chunk 41...\n",
      "Processing chunk 42...\n",
      "Processing chunk 43...\n",
      "Processing chunk 44...\n",
      "Processing chunk 45...\n",
      "Processing chunk 46...\n",
      "Processing chunk 47...\n",
      "Processing chunk 48...\n",
      "Processing chunk 49...\n",
      "Processing chunk 50...\n",
      "Processing chunk 51...\n",
      "Processing chunk 52...\n",
      "Processing chunk 53...\n",
      "Processing chunk 54...\n",
      "Processing chunk 55...\n",
      "Processing chunk 56...\n",
      "Processing chunk 57...\n",
      "Processing chunk 58...\n",
      "Processing chunk 59...\n",
      "Processing chunk 60...\n",
      "Processing chunk 61...\n",
      "Processing chunk 62...\n",
      "Processing chunk 63...\n",
      "Processing chunk 64...\n",
      "Processing chunk 65...\n",
      "Processing chunk 66...\n",
      "Processing chunk 67...\n",
      "Processing chunk 68...\n",
      "Processing chunk 69...\n",
      "Processing chunk 70...\n",
      "Processing chunk 71...\n",
      "Processing chunk 72...\n",
      "Processing chunk 73...\n",
      "Processing chunk 74...\n",
      "Processing chunk 75...\n",
      "Processing chunk 76...\n",
      "Processing chunk 77...\n",
      "Processing chunk 78...\n",
      "Processing chunk 79...\n",
      "Processing chunk 80...\n",
      "Processing chunk 81...\n",
      "Processing chunk 82...\n",
      "Processing chunk 83...\n",
      "Processing chunk 84...\n",
      "Processing chunk 85...\n",
      "Processing chunk 86...\n",
      "Processing chunk 87...\n",
      "Processing chunk 88...\n",
      "Processing chunk 89...\n",
      "Processing chunk 90...\n",
      "Processing chunk 91...\n",
      "Processing chunk 92...\n",
      "Processing chunk 93...\n",
      "Processing chunk 94...\n",
      "Processing chunk 95...\n",
      "Processing chunk 96...\n"
     ]
    }
   ],
   "source": [
    "classified_results = process_all_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47845070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change tag\n",
    "c = 0\n",
    "s = 5347                             \n",
    "for i, m in enumerate(classified_results[s:]):\n",
    "    if m['topic'] == 'unknown':\n",
    "        print(s +i, m['text'])\n",
    "        print(\"********************\")\n",
    "        c += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "45c48b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Batch 1 (Chunks 0 to 29) ---\n",
      "Error during synthesis of chunk 30: Expecting ',' delimiter: line 3 column 122 (char 134)\n",
      "\n",
      "--- Processing Batch 2 (Chunks 30 to 59) ---\n",
      "\n",
      "--- Processing Batch 3 (Chunks 60 to 89) ---\n",
      "Error during synthesis of chunk 90: Expecting value: line 3 column 20 (char 33)\n",
      "\n",
      "--- Processing Batch 4 (Chunks 90 to 119) ---\n",
      "Error during synthesis of chunk 120: Expecting value: line 3 column 20 (char 33)\n",
      "\n",
      "--- Processing Batch 5 (Chunks 120 to 149) ---\n",
      "Error during synthesis of chunk 150: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 6 (Chunks 150 to 179) ---\n",
      "\n",
      "--- Processing Batch 7 (Chunks 180 to 209) ---\n",
      "Error during synthesis of chunk 210: Expecting ',' delimiter: line 8 column 91 (char 361)\n",
      "\n",
      "--- Processing Batch 8 (Chunks 210 to 239) ---\n",
      "Error during synthesis of chunk 240: Expecting ',' delimiter: line 58 column 80 (char 2329)\n",
      "\n",
      "--- Processing Batch 9 (Chunks 240 to 269) ---\n",
      "Error during synthesis of chunk 270: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 10 (Chunks 270 to 299) ---\n",
      "\n",
      "--- Processing Batch 11 (Chunks 300 to 329) ---\n",
      "\n",
      "--- Processing Batch 12 (Chunks 330 to 359) ---\n",
      "Error during synthesis of chunk 360: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 13 (Chunks 360 to 389) ---\n",
      "Error during synthesis of chunk 390: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 14 (Chunks 390 to 419) ---\n",
      "Error during synthesis of chunk 420: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 15 (Chunks 420 to 449) ---\n",
      "Error during synthesis of chunk 450: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 16 (Chunks 450 to 479) ---\n",
      "Error during synthesis of chunk 480: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 17 (Chunks 480 to 509) ---\n",
      "Error during synthesis of chunk 510: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 18 (Chunks 510 to 539) ---\n",
      "Error during synthesis of chunk 540: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 19 (Chunks 540 to 569) ---\n",
      "Error during synthesis of chunk 570: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 20 (Chunks 570 to 599) ---\n",
      "Error during synthesis of chunk 600: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 21 (Chunks 600 to 629) ---\n",
      "Error during synthesis of chunk 630: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 22 (Chunks 630 to 659) ---\n",
      "Error during synthesis of chunk 660: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 23 (Chunks 660 to 689) ---\n",
      "Error during synthesis of chunk 690: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 24 (Chunks 690 to 719) ---\n",
      "Error during synthesis of chunk 720: Expecting ',' delimiter: line 3 column 111 (char 125)\n",
      "\n",
      "--- Processing Batch 25 (Chunks 720 to 749) ---\n",
      "Error during synthesis of chunk 750: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 26 (Chunks 750 to 779) ---\n",
      "Error during synthesis of chunk 780: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 27 (Chunks 780 to 809) ---\n",
      "Error during synthesis of chunk 810: Expecting value: line 3 column 20 (char 34)\n",
      "\n",
      "--- Processing Batch 28 (Chunks 810 to 821) ---\n",
      "Error during synthesis of chunk 822: Expecting value: line 3 column 20 (char 34)\n"
     ]
    }
   ],
   "source": [
    "c = aggregate_topic_chunks(classified_results, time_window_minutes=120)\n",
    "rag_dataset, errors = synthesize_data(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8241bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change case of error (ראשי תיבות)\n",
    "j = -1\n",
    "for i, row in enumerate(new_errors[j].split('\\n')):\n",
    "    print(f\"{i + 1}) {row}\")\n",
    "    \n",
    "synthesized_data = json.loads(new_errors[j].replace('דיג\"יי', \"DJ\").replace('ש\"ח', 'שקל'))\n",
    "            \n",
    "for i, row in synthesized_data.items():\n",
    "    curr_chunk = c[int(i)]\n",
    "    rag_dataset.append({\n",
    "        \"source_topic\": curr_chunk['topic'],\n",
    "        \"summary_text\": row['summary'], # This is your vector content\n",
    "        \"all_names\": row['all_names'], # Filterable fields\n",
    "        \"locations\": row['locations'],\n",
    "        \"original_msg\": curr_chunk['raw_text'], \n",
    "        \"timing\": curr_chunk['timing']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "3cd92c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rag_dataset)\n",
    "df.to_csv(r'C:\\Users\\User\\Documents\\Wedplanner\\Datasets\\bridetobride.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
